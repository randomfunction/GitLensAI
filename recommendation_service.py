import os
import torch
import uvicorn
import numpy as np
from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
import requests
from functools import lru_cache
import logging
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import json

# --- Logging Configuration ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Configuration ---
GITHUB_API_URL = "https://api.github.com"
TOKEN = os.environ.get("GITHUB_TOKEN")
HEADERS = {"Authorization": f"token {TOKEN}"} if TOKEN else {}
if not TOKEN:
    logger.warning("GITHUB_TOKEN environment variable not set. API requests may be rate-limited.")

# --- Device Configuration ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"Using device: {device}")

# --- Model and Data Loading ---
try:
    logger.info("Loading models and data files...")
    
    # Load SentenceTransformer model
    text_model = SentenceTransformer("all-MiniLM-L6-v2")

    # Load GNN embeddings and names from repo_embeddings2.pt
    gnn_data = torch.load("repo_embeddings2.pt", map_location=device)
    if not isinstance(gnn_data, dict) or 'embeddings' not in gnn_data or 'repo_names' not in gnn_data:
        raise ValueError("Invalid structure in 'repo_embeddings2.pt'. Expected a dict with 'embeddings' and 'repo_names'.")
    
    gnn_repo_names = gnn_data['repo_names']
    gnn_embs = gnn_data['embeddings'].cpu().numpy() # For scikit-learn cosine_similarity

    # Load text embeddings generated by build_text_embeddings.py
    text_embs = np.load("text_embeddings.npy")

    # Load repo index generated by build_text_embeddings.py
    with open("repo_index.json", "r") as f:
        repo_index = json.load(f)

    logger.info("Successfully loaded all models and data files.")

except FileNotFoundError as e:
    logger.error(f"FATAL: A required data file was not found: {e}. Please run the build_text_embeddings.py script first.")
    exit()
except Exception as e:
    logger.error(f"An unexpected error occurred during model loading: {e}")
    exit()


# --- FastAPI App ---
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Core Logic ---

@lru_cache(maxsize=128)
def fetch_user_repos(username: str):
    """Fetches a user's public and starred repository data from GitHub."""
    logger.info(f"Fetching GitHub repos for user: {username}")
    repos = []
    
    # Fetch public repos
    repos_url = f"{GITHUB_API_URL}/users/{username}/repos?per_page=100"
    try:
        response = requests.get(repos_url, headers=HEADERS)
        response.raise_for_status()
        repos.extend(response.json())
        logger.info(f"Found {len(response.json())} public repos for {username}.")
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 404:
            raise HTTPException(status_code=404, detail=f"GitHub user '{username}' not found.")
        logger.error(f"Failed to fetch user repos from GitHub: {e}")
        # Don't raise here, we might still have starred repos
    
    # Fetch starred repos
    starred_url = f"{GITHUB_API_URL}/users/{username}/starred?per_page=100"
    try:
        response = requests.get(starred_url, headers=HEADERS)
        response.raise_for_status()
        repos.extend(response.json())
        logger.info(f"Found {len(response.json())} starred repos for {username}.")
    except requests.exceptions.HTTPError as e:
        logger.warning(f"Could not fetch starred repos for {username} (or user has none): {e}")

    return repos

def map_user_repos_to_dataset(user_repos, top_k_map=3):
    """Maps user's GitHub repos to the most similar repos in our dataset using text descriptions."""
    descs = [(r.get("full_name", ""), r.get("description") or "") for r in user_repos]
    texts = [d[1] for d in descs if d[1]] # Only use repos with descriptions
    
    if not texts:
        logger.warning("User's repos have no descriptions to map from.")
        return []

    logger.info(f"Mapping {len(texts)} user repos with descriptions to the dataset.")
    user_text_embs = text_model.encode(texts, show_progress_bar=False)
    
    # Find similarity between user repo descriptions and all dataset repo descriptions
    sims = cosine_similarity(user_text_embs, text_embs)
    
    mapped_repo_names = set()
    for i in range(len(texts)):
        # Get the top N most similar dataset repos for each user repo
        top_dataset_indices = np.argsort(sims[i])[::-1][:top_k_map]
        for idx in top_dataset_indices:
            mapped_repo_names.add(repo_index[idx]["Name"])
            
    logger.info(f"Mapped user to {len(mapped_repo_names)} unique dataset repos.")
    return list(mapped_repo_names)

def recommend_for_username(username: str, top_k=10):
    """Generates repository recommendations for a given username."""
    user_repos = fetch_user_repos(username)
    
    # Stage 1: Map user repos to our dataset via description similarity
    mapped_names = map_user_repos_to_dataset(user_repos, top_k_map=3)
    print(mapped_names)
    
    # Get the GNN embeddings for the mapped repos
    mapped_indices = [i for i, name in enumerate(gnn_repo_names) if name in mapped_names]
    print(mapped_indices)
    
    if not mapped_indices:
        logger.warning(f"Could not map any of {username}'s repos to the GNN dataset.")
        return []

    # Stage 2: Create a user vector and find similar repos in GNN space
    user_vector = np.mean(gnn_embs[mapped_indices], axis=0, keepdims=True)
    print(user_vector)
    
    # Calculate similarity between user vector and all GNN embeddings
    gnn_sims = cosine_similarity(user_vector, gnn_embs)[0]
    print(gnn_sims)
    
    # Get top recommendations, excluding repos the user is already mapped to
    ordered_indices = np.argsort(gnn_sims)[::-1]
    print(ordered_indices)
    
    recommendations = []
    user_original_repo_names = {r.get("name") for r in user_repos}
    print(user_original_repo_names)
    

    for idx in ordered_indices:
        repo_name = gnn_repo_names[idx]
        # Exclude repos the user already has or that we used for mapping
        if repo_name not in mapped_names and repo_name not in user_original_repo_names:
            recommendations.append({
                "repo": repo_name,
                "score": float(gnn_sims[idx])
            })
        if len(recommendations) >= top_k:
            break
            
    logger.info(f"Generated {len(recommendations)} recommendations for {username}.")
    print(recommendations)
    return recommendations


# --- API Endpoint ---
@app.get("/recommend")
def recommend(username: str = Query(..., description="GitHub username to generate recommendations for."), 
              top_k: int = Query(10, description="The number of recommendations to return.")):
    try:
        results = recommend_for_username(username, top_k=top_k)
        print(results)
        if not results:
            raise HTTPException(status_code=404, detail="Could not generate recommendations. The user may have no repositories, or none that could be mapped to the dataset.")
        return {"username": username, "recommendations": results}
    except HTTPException as e:
        # Re-raise HTTP exceptions from sub-functions
        raise e
    except Exception as e:
        logger.error(f"An unexpected error occurred in the /recommend endpoint for user {username}: {e}")
        raise HTTPException(status_code=500, detail="An internal error occurred while generating recommendations.")

# --- Main Execution ---
if __name__ == "__main__":
    logger.info("Starting FastAPI server...")
    uvicorn.run(app, host="0.0.0.0", port=8000)
